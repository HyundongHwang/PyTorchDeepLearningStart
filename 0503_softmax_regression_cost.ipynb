{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myutil as mu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset  # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader  # 데이터로더\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt  # 맷플롯립사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 파이토치로 소프트맥스의 비용 함수 구현하기 (로우-레벨) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "z : \n",
      "    torch.Size([3]) tensor([1., 2., 3.])\n",
      "\n",
      "hypothesis : \n",
      "    torch.Size([3]) tensor([0.0900, 0.2447, 0.6652])\n",
      "\n",
      "hypothesis.sum() : \n",
      "    torch.Size([]) 1.0\n",
      "\n",
      "z : \n",
      "    torch.Size([3, 5]) tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],\n",
      "            [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],\n",
      "            [0.6387, 0.5247, 0.6826, 0.3051, 0.4635]])\n",
      "\n",
      "hypothesis : \n",
      "    torch.Size([3, 5]) tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "            [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "            [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]])\n",
      "\n",
      "hypothesis.sum(dim=1) : \n",
      "    torch.Size([3]) tensor([1.0000, 1.0000, 1.0000])\n",
      "\n",
      "y : \n",
      "    torch.Size([3]) tensor([0, 2, 1])\n",
      "\n",
      "hypothesis : \n",
      "    torch.Size([3, 5]) tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "            [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "            [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]])\n",
      "\n",
      "y_one_hot : \n",
      "    torch.Size([3, 5]) tensor([[0., 0., 0., 0., 0.],\n",
      "            [0., 0., 0., 0., 0.],\n",
      "            [0., 0., 0., 0., 0.]])\n",
      "\n",
      "y.unsqueeze(1) : \n",
      "    torch.Size([3, 1]) tensor([[0],\n",
      "            [2],\n",
      "            [1]])\n",
      "\n",
      "y_one_hot.scatter_(dim=1, y.unsqueeze(dim=1), index=1) : \n",
      "    torch.Size([3, 5]) tensor([[1., 0., 0., 0., 0.],\n",
      "            [0., 0., 1., 0., 0.],\n",
      "            [0., 1., 0., 0., 0.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "z = torch.FloatTensor([1, 2, 3])\n",
    "hypothesis = F.softmax(z, dim=0)\n",
    "mu.log(\"z\", z)\n",
    "mu.log(\"hypothesis\", hypothesis)\n",
    "mu.log(\"hypothesis.sum()\", hypothesis.sum())\n",
    "\n",
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "mu.log(\"z\", z)\n",
    "hypothesis = F.softmax(z, dim=1)\n",
    "mu.log(\"hypothesis\", hypothesis)\n",
    "mu.log(\"hypothesis.sum(dim=1)\", hypothesis.sum(dim=1))\n",
    "\n",
    "y = torch.randint(5, (3,))\n",
    "mu.log(\"y\", y)\n",
    "hypothesis = F.softmax(z, dim=1)\n",
    "mu.log(\"hypothesis\", hypothesis)\n",
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "mu.log(\"y_one_hot\", y_one_hot)\n",
    "mu.log(\"y.unsqueeze(1)\", y.unsqueeze(1))\n",
    "mu.log(\"y_one_hot.scatter_(dim=1, y.unsqueeze(dim=1), index=1)\", y_one_hot.scatter_(1, y.unsqueeze(1), 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 이제 비용 함수 연산을 위한 재료들을 전부 손질했습니다. \n",
    " - 소프트맥스 회귀의 비용 함수는 다음과 같았습니다. \n",
    " ![](https://render.githubusercontent.com/render/math?math=cost(W)%20=%20-\\frac{1}{n}%20\\sum_{i=1}^{n}%20\\sum_{j=1}^{k}y_{j}^{(i)}\\%20log(p_{j}^{(i)})) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : \n",
      "    torch.Size([]) 1.468920350074768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "mu.log(\"cost\", cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 파이토치로 소프트맥스의 비용 함수 구현하기 (하이-레벨) \n",
    " - 이제 소프트맥스의 비용 함수를 좀 더 하이-레벨로 구현하는 방법에 대해서 알아봅시다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z : \n",
      "    torch.Size([3, 5]) tensor([[0.9371, 0.6556, 0.3138, 0.1980, 0.4162],\n",
      "            [0.2843, 0.3398, 0.5239, 0.7981, 0.7718],\n",
      "            [0.0112, 0.8100, 0.6397, 0.9743, 0.8300]])\n",
      "\n",
      "y : \n",
      "    torch.Size([3]) tensor([3, 2, 3])\n",
      "\n",
      "y_one_hot : \n",
      "    torch.Size([3, 5]) tensor([[0., 0., 0., 1., 0.],\n",
      "            [0., 0., 1., 0., 0.],\n",
      "            [0., 0., 0., 1., 0.]])\n",
      "\n",
      "cost low level : \n",
      "    torch.Size([]) 1.6471147537231445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "mu.log(\"z\", z)\n",
    "y = torch.randint(5, (3,))\n",
    "mu.log(\"y\", y)\n",
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)\n",
    "mu.log(\"y_one_hot\", y_one_hot)\n",
    "hypothesis = F.softmax(z, dim=1)\n",
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "mu.log(\"cost low level\", cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - F.softmax() + torch.log() = F.log_softmax() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost log_softmax : \n",
      "    torch.Size([]) 1.6471147537231445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost = (y_one_hot * -F.log_softmax(z, dim=1)).sum(dim=1).mean()\n",
    "mu.log(\"cost log_softmax\", cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 여기서 nll이란 Negative Log Likelihood의 약자입니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost nll_loss log_softmax : \n",
      "    torch.Size([]) 1.6471147537231445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost = F.nll_loss(F.log_softmax(z, dim=1), y)\n",
    "mu.log(\"cost nll_loss log_softmax\", cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - F.log_softmax() + F.nll_loss() = F.cross_entropy() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost cross_entropy : \n",
      "    torch.Size([]) 1.6471147537231445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost = F.cross_entropy(z, y)\n",
    "mu.log(\"cost cross_entropy\", cost)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

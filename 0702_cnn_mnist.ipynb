{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myutil as mu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset  # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader  # 데이터로더\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt  # 맷플롯립사용\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from sklearn.datasets import load_digits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - CNN으로 MNIST 분류하기 \n",
    "   - 이번 챕터에서는 CNN으로 MNIST를 분류해보겠습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " 임의의 텐서를 만듭니다. 텐서의 크기는 1 × 1 × 28 × 28입니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inputs : \n",
      "    torch.Size([1, 1, 28, 28]) tensor([[[[-5.7412e+07,  4.5712e-41,  1.5414e-44,  0.0000e+00,  0.0000e+00,\n",
      "                0.0000e+00,  ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.Tensor(1, 1, 28, 28)\n",
    "mu.log(\"inputs\", inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 합성곱층과 풀링 선언하기 \n",
    "   - 이제 첫번째 합성곱 층을 구현해봅시다. \n",
    "   - 1채널 짜리를 입력받아서 32채널을 뽑아내는데 커널 사이즈는 3이고 패딩은 1입니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 : \n",
      "    Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) \n",
      "    torch.Size([32, 1, 3, 3]) tensor([[[[ 0.1136, -0.3329, -0.2063],\n",
      "              [-0.2326, -0.0497, -0.2653],\n",
      "              [ 0.2389, -0 ...\n",
      "    torch.Size([32]) tensor([-0.2414,  0.2156, -0.0568,  0.2838,  0.1000,  0.0281, -0.0484,  0.0964,\n",
      "            -0.2412, -0. ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), padding=(1, 1))\n",
    "mu.log(\"conv1\", conv1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 이제 두번째 합성곱 층을 구현해봅시다. \n",
    "   - 32채널 짜리를 입력받아서 64채널을 뽑아내는데 커널 사이즈는 3이고 패딩은 1입니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2 : \n",
      "    Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) \n",
      "    torch.Size([64, 32, 3, 3]) tensor([[[[ 5.8330e-02,  2.6579e-02,  4.1431e-02],\n",
      "              [-3.1966e-02, -1.0294e-02,  5.2576e-02] ...\n",
      "    torch.Size([64]) tensor([-3.8194e-02,  2.2961e-02, -3.6868e-02, -3.9716e-02,  5.1850e-02,\n",
      "             3.3440e-02, -4.684 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=(1, 1))\n",
    "mu.log(\"conv2\", conv2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 이제 맥스풀링을 구현해봅시다. \n",
    "   - 정수 하나를 인자로 넣으면 커널 사이즈와 스트라이드가 둘 다 해당값으로 지정됩니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool : \n",
      "    MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "mu.log(\"pool\", pool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 구현체를 연결하여 모델 만들기 \n",
    "   - 지금까지는 선언만한 것이고 아직 이들을 연결시키지는 않았습니다. \n",
    "   - 이들을 연결시켜서 모델을 완성시켜보겠습니다. \n",
    "   - 우선 입력을 첫번째 합성곱층을 통과시키고 합성곱층을 통과시킨 후의 텐서의 크기를 보겠습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = conv1(inputs) : \n",
      "    torch.Size([1, 32, 28, 28]) tensor([[[[ 2.8533e+06,         nan,         nan,  ..., -2.4139e-01,\n",
      "               -2.4139e-01, -2.4139 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = conv1(inputs)\n",
    "mu.log(\"out = conv1(inputs)\", out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 32채널의 28너비 28높이의 텐서가 되었습니다. \n",
    " - 32가 나온 이유는 conv1의 out_channel로 32를 지정해주었기 때문입니다. \n",
    " - 또한, 28너비 28높이가 된 이유는 패딩을 1폭으로 하고 3 × 3 커널을 사용하면 크기가 보존되기 때문입니다. \n",
    " - 이제 이를 맥스풀링을 통과시키고 맥스풀링을 통과한 후의 텐서의 크기를 보겠습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = pool(out) : \n",
      "    torch.Size([1, 32, 14, 14]) tensor([[[[        nan,         nan,         nan,  ...,         nan,\n",
      "               -2.4139e-01, -2.4139 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = pool(out)\n",
    "mu.log(\"out = pool(out)\", out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 32채널의 14너비 14높이의 텐서가 되었습니다. \n",
    " - 이제 이를 다시 두번째 합성곱층에 통과시키고 통과한 후의 텐서의 크기를 보겠습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = conv2(out) : \n",
      "    torch.Size([1, 64, 14, 14]) tensor([[[[        nan,         nan,         nan,  ...,         nan,\n",
      "                       nan,  1.0534 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = conv2(out)\n",
    "mu.log(\"out = conv2(out)\", out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 64채널의 14너비 14높이의 텐서가 되었습니다. \n",
    " - 64가 나온 이유는 conv2의 out_channel로 64를 지정해주었기 때문입니다. \n",
    " - 또한, 14너비 14높이가 된 이유는 패딩을 1폭으로 하고 3 × 3 커널을 사용하면 크기가 보존되기 때문입니다. \n",
    " - 이제 이를 맥스풀링을 통과시키고 맥스풀링을 통과한 후의 텐서의 크기를 보겠습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = pool(out) : \n",
      "    torch.Size([1, 64, 7, 7]) tensor([[[[        nan,         nan,         nan,  ...,         nan,\n",
      "                       nan,         ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = pool(out)\n",
    "mu.log(\"out = pool(out)\", out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 이제 이 텐서를 펼치는 작업을 할 겁니다. \n",
    " - 그런데 펼치기에 앞서 텐서의 n번째 차원을 접근하게 해주는 .size(n)에 대해서 배워보겠습니다. \n",
    " - 현재 out의 크기는 1 × 64 × 7 × 7입니다. \n",
    " - out의 첫번째 차원이 몇인지 출력해보겠습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.size(0) : 1\n",
      "out.size(1) : 64\n",
      "out.size(2) : 7\n",
      "out.size(3) : 7\n"
     ]
    }
   ],
   "source": [
    "mu.log(\"out.size(0)\", out.size(0))\n",
    "mu.log(\"out.size(1)\", out.size(1))\n",
    "mu.log(\"out.size(2)\", out.size(2))\n",
    "mu.log(\"out.size(3)\", out.size(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " 이제 이를 가지고 .view()를 사용하여 텐서를 펼치는 작업을 해보겠습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "out = out.view(out.size(0), -1) : \n",
      "    torch.Size([1, 3136]) tensor([[       nan,        nan,        nan,  ..., 7.1078e+34, 6.3845e+36,\n",
      "             4.4825e+36]])\n",
      "\n",
      "out.size(1) : 3136\n"
     ]
    }
   ],
   "source": [
    "out = out.view(out.size(0), -1)\n",
    "mu.log(\"out = out.view(out.size(0), -1)\", out)\n",
    "mu.log(\"out.size(1)\", out.size(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 배치 차원을 제외하고 모두 하나의 차원으로 통합된 것을 볼 수 있습니다. \n",
    " - 이제 이에 대해서 전결합층(Fully-Connteced layer)를 통과시켜보겠습니다. \n",
    " - 출력층으로 10개의 뉴런을 배치하여 10개 차원의 텐서로 변환합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fc : \n",
      "    Linear(in_features=3136, out_features=10, bias=True) \n",
      "    torch.Size([10, 3136]) tensor([[ 0.0029,  0.0118,  0.0160,  ..., -0.0109, -0.0010, -0.0068],\n",
      "            [-0.0124,  0.0145,  0. ...\n",
      "    torch.Size([10]) tensor([-0.0104, -0.0138, -0.0080,  0.0147, -0.0151, -0.0090,  0.0090,  0.0094,\n",
      "            -0.0169,  0. ...\n",
      "\n",
      "out = fc(out) : \n",
      "    torch.Size([1, 10]) tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(out.size(1), 10)\n",
    "mu.log(\"fc\", fc)\n",
    "out = fc(out)\n",
    "mu.log(\"out = fc(out)\", out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " CNN으로 MNIST 분류하기 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "torch.manual_seed(777)\n",
    "\n",
    "# GPU 사용 가능일 경우 랜덤 시드 고정\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " 학습에 사용할 파라미터를 설정합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " 데이터로더를 사용하여 데이터를 다루기 위해서 데이터셋을 정의해줍니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_train : \n",
      "    Dataset MNIST\n",
      "        Number of datapoints: 60000\n",
      "        Root location: MNIST_data/\n",
      "        Split: Train\n",
      "        StandardTransform\n",
      "    Transform: ToTensor()\n",
      "\n",
      "mnist_test : \n",
      "    Dataset MNIST\n",
      "        Number of datapoints: 10000\n",
      "        Root location: MNIST_data/\n",
      "        Split: Test\n",
      "        StandardTransform\n",
      "    Transform: ToTensor()\n",
      "\n",
      "len(data_loader) : 600\n",
      "data_loader.sampler.num_samples : 60000\n",
      "data_loader.batch_size : 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',  # 다운로드 경로 지정\n",
    "                          train=True,  # True를 지정하면 훈련 데이터로 다운로드\n",
    "                          transform=transforms.ToTensor(),  # 텐서로 변환\n",
    "                          download=True)\n",
    "\n",
    "mu.log(\"mnist_train\", mnist_train)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',  # 다운로드 경로 지정\n",
    "                         train=False,  # False를 지정하면 테스트 데이터로 다운로드\n",
    "                         transform=transforms.ToTensor(),  # 텐서로 변환\n",
    "                         download=True)\n",
    "\n",
    "mu.log(\"mnist_test\", mnist_test)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "mu.log(\"len(data_loader)\", len(data_loader))\n",
    "mu.log(\"data_loader.sampler.num_samples\", data_loader.sampler.num_samples)\n",
    "mu.log(\"data_loader.batch_size\", data_loader.batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " 이제 클래스로 모델을 설계합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 첫번째층\n",
    "        # ImgIn shape=(?, 28, 28, 1)\n",
    "        #    Conv     -> (?, 28, 28, 32)\n",
    "        #    Pool     -> (?, 14, 14, 32)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # 두번째층\n",
    "        # ImgIn shape=(?, 14, 14, 32)\n",
    "        #    Conv      ->(?, 14, 14, 64)\n",
    "        #    Pool      ->(?, 7, 7, 64)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # 전결합층 7x7x64 inputs -> 10 outputs\n",
    "        self.fc = nn.Linear(in_features=7 * 7 * 64, out_features=10, bias=True)\n",
    "\n",
    "        # 전결합층 한정으로 가중치 초기화\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        # 전결합층을 위해서 Flatten\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " 모델을 정의합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model : \n",
      "    CNN(\n",
      "      (layer1): Sequential(\n",
      "        (0): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      "    ) \n",
      "    torch.Size([32, 1, 2, 2]) tensor([[[[-0.4181, -0.0089],\n",
      "              [-0.0967, -0.1141]]],\n",
      "    \n",
      "    \n",
      "            [[[ 0.3813,  0.3811],\n",
      "           ...\n",
      "    torch.Size([32]) tensor([ 0.3389,  0.3638, -0.1973,  0.1962, -0.4985, -0.2323,  0.1264,  0.3711,\n",
      "            -0.2138,  0. ...\n",
      "    torch.Size([64, 32, 2, 2]) tensor([[[[-0.0219,  0.0857],\n",
      "              [ 0.0353, -0.0390]],\n",
      "    \n",
      "             [[ 0.0069,  0.0461],\n",
      "             ...\n",
      "    torch.Size([64]) tensor([-0.0171, -0.0457,  0.0134, -0.0840, -0.0716,  0.0616,  0.0698, -0.0761,\n",
      "             0.0017, -0. ...\n",
      "    torch.Size([10, 3136]) tensor([[ 0.0283,  0.0206, -0.0291,  ..., -0.0410, -0.0045, -0.0198],\n",
      "            [-0.0059,  0.0081, -0. ...\n",
      "    torch.Size([10]) tensor([ 0.0108, -0.0013, -0.0176,  0.0151,  0.0092,  0.0141, -0.0037,  0.0007,\n",
      "            -0.0128, -0. ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "mu.log(\"model\", model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " 비용 함수와 옵티마이저를 정의합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion : \n",
      "    CrossEntropyLoss() \n",
      "\n",
      "optimizer : \n",
      "    Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 0.001\n",
      "        weight_decay: 0\n",
      "    )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "mu.log(\"criterion\", criterion)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "mu.log(\"optimizer\", optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " 총 배치의 수를 출력해보겠습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_batch : 600\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "mu.log(\"total_batch\", total_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 총 배치의 수는 600입니다. \n",
    " - 그런데 배치 크기를 100으로 했으므로 결국 훈련 데이터는 총 60,000개란 의미입니다. \n",
    " - 이제 모델을 훈련시켜보겠습니다. \n",
    " - (시간이 꽤 오래 걸립니다.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------- \n",
      "epoch :    0/15 \n",
      "cost : 0.286877 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :    1/15 \n",
      "cost : 0.081790 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :    2/15 \n",
      "cost : 0.062068 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :    3/15 \n",
      "cost : 0.052168 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :    4/15 \n",
      "cost : 0.044108 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :    5/15 \n",
      "cost : 0.039927 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :    6/15 \n",
      "cost : 0.034997 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :    7/15 \n",
      "cost : 0.030174 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :    8/15 \n",
      "cost : 0.026860 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :    9/15 \n",
      "cost : 0.023716 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :   10/15 \n",
      "cost : 0.020913 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :   11/15 \n",
      "cost : 0.018844 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :   12/15 \n",
      "cost : 0.016937 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :   13/15 \n",
      "cost : 0.015007 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :   14/15 \n",
      "cost : 0.012584 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "epoch :   15/15 \n",
      "cost : 0.011297 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mu.plt_init()\n",
    "\n",
    "for epoch in range(training_epochs + 1):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    mu.log_epoch(epoch, training_epochs, avg_cost)\n",
    "\n",
    "mu.plt_show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 이제 테스트를 해보겠습니다. \n",
    " -98%의 정확도를 얻습니다. 다음 챕터에서는 층을 더 쌓아보겠습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy : \n",
      "    torch.Size([]) 0.9520000219345093\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 학습을 진행하지 않을 것이므로 torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    mu.log(\"accuracy\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

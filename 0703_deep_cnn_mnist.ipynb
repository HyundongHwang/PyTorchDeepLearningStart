{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import myutil as mu\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import TensorDataset  # 텐서데이터셋\n",
                "from torch.utils.data import DataLoader  # 데이터로더\n",
                "from torch.utils.data import Dataset\n",
                "import matplotlib.pyplot as plt  # 맷플롯립사용\n",
                "import torchvision.datasets as dsets\n",
                "import torchvision.transforms as transforms\n",
                "from torch.utils.data import DataLoader\n",
                "import random\n",
                "from sklearn.datasets import load_digits\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 깊은 CNN으로 MNIST 분류하기 \n",
                "   - 이번 챕터에서는 앞서 배운 CNN에 층을 더 추가하여 MNIST를 분류해보겠습니다. \n",
                "   - 우리가 만들 모델의 아키텍처를 이해해봅시다. \n",
                "   - 모델의 아키텍처는 총 5개의 층으로 구성됩니다. \n",
                "   - 앞서 배운 챕터에서 1번 레이어와 2번 레이어는 동일하되, 새로운 합성곱층과 전결합층을 추가했습니다. \n",
                "   - 사실 이번 챕터의 코드는 이전 챕터에서 층이 조금 더 추가되는 것 말고는 동일합니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "\n",
                "# 랜덤 시드 고정\n",
                "torch.manual_seed(777)\n",
                "\n",
                "# GPU 사용 가능일 경우 랜덤 시드 고정\n",
                "if device == 'cuda':\n",
                "    torch.cuda.manual_seed_all(777)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " 학습에 사용할 파라미터를 설정합니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "learning_rate = 0.001\n",
                "training_epochs = 15\n",
                "batch_size = 100\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " 데이터로더를 사용하여 데이터를 다루기 위해서 데이터셋을 정의해줍니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "mnist_train = dsets.MNIST(root='MNIST_data/',  # 다운로드 경로 지정\n",
                "                          train=True,  # True를 지정하면 훈련 데이터로 다운로드\n",
                "                          transform=transforms.ToTensor(),  # 텐서로 변환\n",
                "                          download=True)\n",
                "\n",
                "mu.log(\"mnist_train\", mnist_train)\n",
                "\n",
                "mnist_test = dsets.MNIST(root='MNIST_data/',  # 다운로드 경로 지정\n",
                "                         train=False,  # False를 지정하면 테스트 데이터로 다운로드\n",
                "                         transform=transforms.ToTensor(),  # 텐서로 변환\n",
                "                         download=True)\n",
                "\n",
                "mu.log(\"mnist_test\", mnist_test)\n",
                "\n",
                "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
                "                                          batch_size=batch_size,\n",
                "                                          shuffle=True,\n",
                "                                          drop_last=True)\n",
                "\n",
                "mu.log(\"len(data_loader)\", len(data_loader))\n",
                "mu.log(\"data_loader.sampler.num_samples\", data_loader.sampler.num_samples)\n",
                "mu.log(\"data_loader.batch_size\", data_loader.batch_size)\n",
                "\n",
                "\n",
                "class CNN(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(CNN, self).__init__()\n",
                "        self.keep_probe = 0.5\n",
                "\n",
                "        # (?=데이타갯, 28=width, 28=height, 1=채널수)\n",
                "\n",
                "        self.layer1 = nn.Sequential(\n",
                "            # 패딩 1을 줘서 width, height 변화가 없음\n",
                "            # out_channels를 32로 늘려서 채널수가 증폭되었음.\n",
                "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
                "            # (?, 28, 28, 32)\n",
                "\n",
                "            # ReLU는 shape에 영향없음.\n",
                "            nn.ReLU(),\n",
                "            # (?, 28, 28, 32)\n",
                "\n",
                "            # MaxPool(kernel_size=2, stride=2)을 통해서 width, height가 절반으로 줄었음.\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "            # (?, 14, 14, 32)\n",
                "        )\n",
                "\n",
                "        self.layer2 = nn.Sequential(\n",
                "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
                "            # (?, 14, 14, 64)\n",
                "\n",
                "            nn.ReLU(),\n",
                "            # (?, 14, 14, 64)\n",
                "\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "            # (?, 7, 7, 32)\n",
                "        )\n",
                "\n",
                "        self.layer3 = nn.Sequential(\n",
                "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
                "            # (?, 7, 7, 128)\n",
                "\n",
                "            nn.ReLU(),\n",
                "            # (?, 7, 7, 128)\n",
                "\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
                "            # (?, 4, 4, 128)\n",
                "        )\n",
                "\n",
                "        # layer3까지의 이미지를 flatten 한 결과\n",
                "        # 즉 (?, 4, 4, 128) -> (?, -1) -> (?, 4 * 4 * 128)\n",
                "        self.fc1 = nn.Linear(4 * 4 * 128, 625, bias=True)\n",
                "\n",
                "        # fc1 값들을 xavier 초기화\n",
                "        nn.init.xavier_uniform_(self.fc1.weight)\n",
                "\n",
                "        self.layer4 = nn.Sequential(\n",
                "            self.fc1,\n",
                "            # (?, 625)\n",
                "\n",
                "            nn.ReLU(),\n",
                "            # (?, 625)\n",
                "\n",
                "            nn.Dropout(p=1 - self.keep_probe)\n",
                "        )\n",
                "\n",
                "        self.fc2 = nn.Linear(625, 10, bias=True)\n",
                "        nn.init.xavier_uniform_(self.fc2.weight)\n",
                "\n",
                "        self.layer5 = nn.Sequential(\n",
                "            self.fc2\n",
                "            # (?, 10)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        out = self.layer1(x)\n",
                "        out = self.layer2(out)\n",
                "        out = self.layer3(out)\n",
                "        data_count = out.size(0)\n",
                "        out = out.view(data_count, -1)\n",
                "        out = self.layer4(out)\n",
                "        out = self.layer5(out)\n",
                "        return out\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " 모델을 정의합니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = CNN().to(device)\n",
                "mu.log(\"model\", model)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " 비용 함수와 옵티마이저를 정의합니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "criterion = nn.CrossEntropyLoss().to(device)\n",
                "mu.log(\"criterion\", criterion)\n",
                "\n",
                "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
                "mu.log(\"optimizer\", optimizer)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " 총 배치의 수를 출력해보겠습니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "total_batch = len(data_loader)\n",
                "mu.log(\"total_batch\", total_batch)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 총 배치의 수는 600입니다. \n",
                " - 그런데 배치 크기를 100으로 했으므로 결국 훈련 데이터는 총 60,000개란 의미입니다. \n",
                " - 이제 모델을 훈련시켜보겠습니다. \n",
                " - (시간이 꽤 오래 걸립니다.) \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "mu.plt_init()\n",
                "\n",
                "for epoch in range(training_epochs + 1):\n",
                "    avg_cost = 0\n",
                "\n",
                "    for X, Y in data_loader:\n",
                "        X = X.to(device)\n",
                "        Y = Y.to(device)\n",
                "\n",
                "        hypothesis = model(X)\n",
                "        cost = criterion(hypothesis, Y)\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        cost.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        avg_cost += cost / total_batch\n",
                "\n",
                "    mu.log_epoch(epoch, training_epochs, avg_cost)\n",
                "\n",
                "mu.plt_show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 이제 테스트를 해보겠습니다. \n",
                "   - 층을 더 깊게 쌓았는데 오히려 정확도가 줄어드는 것을 볼 수 있습니다. \n",
                "   - 결국 층을 깊게 쌓는 것도 중요하지만, 꼭 깊게 쌓는 것이 정확도를 올려주지는 않으며 효율적으로 쌓는 것도 중요하다는 의미입니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# 학습을 진행하지 않을 것이므로 torch.no_grad()\n",
                "with torch.no_grad():\n",
                "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
                "    Y_test = mnist_test.test_labels.to(device)\n",
                "\n",
                "    prediction = model(X_test)\n",
                "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
                "    accuracy = correct_prediction.float().mean()\n",
                "    mu.log(\"accuracy\", accuracy)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 2
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython2",
            "version": "2.7.17"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myutil as mu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 자연어 처리 전처리 이해하기 \n",
    "   - 자연어 처리는 일반적으로 토큰화, 단어 집합 생성, 정수 인코딩, 패딩, 벡터화의 과정을 거칩니다. \n",
    "   - 이번 챕터에서는 이러한 전반적인 과정에 대해서 이해합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - spaCy 사용하기 \n",
    " \n",
    " ``` \n",
    " pip install spacy \n",
    " python3 -m spacy download en \n",
    " ``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy_en : <spacy.lang.en.English object at 0x7f100ed8da30>\n",
      "tokenize(en_text) : ['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "en_text = \"A Dog Run back corner near spare bedrooms\"\n",
    "spacy_en = spacy.load(\"en\")\n",
    "mu.log(\"spacy_en\", spacy_en)\n",
    "\n",
    "\n",
    "def tokenize(en_text):\n",
    "    return [\n",
    "        tok.text\n",
    "        for tok in spacy_en.tokenizer(en_text)\n",
    "    ]\n",
    "\n",
    "\n",
    "mu.log(\"tokenize(en_text)\", tokenize(en_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - NLTK 사용하기 \n",
    " \n",
    " ``` \n",
    " pip install nltk \n",
    " ``` \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize(en_text) : ['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hhd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "mu.log(\"word_tokenize(en_text)\", word_tokenize(en_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 띄어쓰기로 토큰화 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_text.split() : ['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mu.log(\"en_text.split()\", en_text.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 띄어쓰기로 토큰화 \n",
    "   - 위의 예제에서는 '사과'란 단어가 총 4번 등장했는데 \n",
    "   - 모두 '의', '를', '가', '랑' 등이 붙어있어 \n",
    "   - 이를 제거해주지 않으면 기계는 전부 다른 단어로 인식하게 됩니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kor_text.split() : ['사과의', '놀라운', '효능이라는', '글을', '봤어.', '그래서', '오늘', '사과를', '먹으려고', '했는데', '사과가', '썩어서', '슈퍼에', '가서', '사과랑', '오렌지', '사왔어']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\"\n",
    "mu.log(\"kor_text.split()\", kor_text.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 형태소 토큰화 \n",
    "   - 위와 같은 상황을 방지하기 위해서 한국어는 보편적으로 '형태소 분석기'로 토큰화를 합니다. \n",
    "   - 여기서는 형태소 분석기 중에서 mecab을 사용해보겠습니다. \n",
    "   - 아래의 커맨드로 colab에서 mecab을 설치합니다. \n",
    "   - 앞선 예와 다르게 '의', '를', '가', '랑' 등이 전부 분리되어 기계는 '사과'라는 단어를 하나의 단어로 처리할 수 있습니다. \n",
    " \n",
    " ``` \n",
    " git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git \n",
    " cd Mecab-ko-for-Google-Colab \n",
    " chmod u+x install_mecab-ko_on_colab190912.sh \n",
    " ./install_mecab-ko_on_colab190912.sh \n",
    " ``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.morphs(kor_text) : ['사과', '의', '놀라운', '효능', '이', '라는', '글', '을', '봤', '어', '.', '그래서', '오늘', '사과', '를', '먹', '으려고', '했', '는데', '사과', '가', '썩', '어서', '슈퍼', '에', '가', '서', '사과', '랑', '오렌지', '사', '왔', '어']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "tokenizer = Mecab()\n",
    "mu.log(\"tokenizer.morphs(kor_text)\", tokenizer.morphs(kor_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 단어 집합(Vocabulary) 생성 \n",
    "   - 단어 집합(vocabuary)이란 중복을 제거한 텍스트의 총 단어의 집합(set)을 의미합니다. \n",
    "   - 우선, 실습을 위해서 깃허브에서 '네이버 영화 리뷰 분류하기' 데이터를 다운로드하겠습니다. \n",
    "   - 네이버 영화 리뷰 데이터는 총 20만 개의 영화 리뷰를 긍정 1, 부정 0으로 레이블링한 데이터입니다. \n",
    " \n",
    " ``` \n",
    " pip3 install pandas \n",
    " ``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data) : 200000\n",
      "\n",
      "data[:10] : \n",
      "             id                                           document  label\n",
      "    0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
      "    1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
      "    2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
      "    3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
      "    4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
      "    5   2190435                      사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화      1\n",
      "    6   9279041                                   완전 감동입니다 다시봐도 감동      1\n",
      "    7   7865729                        개들의 전쟁2 나오나요? 나오면 1빠로 보고 싶음      1\n",
      "    8   7477618                                                  굿      1\n",
      "    9   9250537                                     바보가 아니라 병 쉰 인듯      1\n",
      "\n",
      "tokenized[:10] : [['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ'], ['디자인', '을', '배우', '학생', ',', '외국', '디자이너', '그', '일군', '전통', '을', '통해', '발전', '해', '문화', '산업', '부러웠', '는데', '.', '사실', '우리', '나라', '에서', '그', '어려운', '시절', '끝', '까지', '열정', '을', '지킨', '노라노', '같', '전통', '있', '어', '저', '같', '사람', '꿈', '을', '꾸', '고', '이뤄나갈', '수', '있', '다는', '것', '감사', '합니다', '.'], ['폴리스', '스토리', '시리즈', '1', '부터', '뉴', '까지', '버릴', '께', '하나', '없', '음', '.', '.', '최고', '.'], ['.', '.', '연기', '진짜', '개', '쩔', '구나', '.', '.', '지루', '할거', '라고', '생각', '했', '는데', '몰입', '해서', '봤', '다', '.', '.', '그래', '이런', '게', '진짜', '영화', '지'], ['안개', '자욱', '밤하늘', '떠', '있', '초승달', '같', '영화', '.'], ['사랑', '을', '해', '본', '사람', '라면', '처음', '부터', '끝', '까지', '웃', '을', '수', '있', '영화'], ['완전', '감동', '입니다', '다시', '봐도', '감동'], ['개', '전쟁', '2', '나오', '나요', '?', '나오', '면', '1', '빠', '로', '보', '고', '싶', '음'], ['굿'], ['바보', '아니', '라', '병', '쉰', '인', '듯']]\n",
      "len(vocab) : 698\n",
      "vocab['재밌'] : 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    url=\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\",\n",
    "    filename=\"ratings.txt\"\n",
    ")\n",
    "\n",
    "data = pd.read_table(\"ratings.txt\")\n",
    "mu.log(\"len(data)\", len(data))\n",
    "mu.log(\"data[:10]\", data[:10])\n",
    "sample_data = data[:100]\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
    "tokenized = []\n",
    "\n",
    "for sentence in sample_data[\"document\"]:\n",
    "    temp = []\n",
    "    temp = tokenizer.morphs(sentence)\n",
    "\n",
    "    temp = [\n",
    "        word\n",
    "        for word in temp\n",
    "        if not word in stopwords\n",
    "    ]\n",
    "\n",
    "    tokenized.append(temp)\n",
    "\n",
    "mu.log(\"tokenized[:10]\", tokenized[:10])\n",
    "\n",
    "vocab = FreqDist(np.hstack(tokenized))\n",
    "mu.log(\"len(vocab)\", len(vocab))\n",
    "mu.log(\"vocab['재밌']\", vocab['재밌'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 각 단어에 고유한 정수 부여 \n",
    "   - enumerate()는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 \n",
    "   - 인덱스를 순차적으로 함께 리턴한다는 특징이 있습니다. \n",
    "   - 인덱스 0과 1은 다른 용도로 남겨두고 나머지 단어들은 2부터 501까지 순차적으로 인덱스를 부여해봅시다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_to_index) : 368\n",
      "len(encoded) : 100\n",
      "encoded[:10] : [[1, 594, 626, 267, 1, 1, 1, 1, 1, 1], [1, 6, 1, 1, 19, 1, 1, 623, 1, 1, 6, 1, 1, 164, 1, 1, 1, 1, 30, 1, 1, 1, 1, 623, 1, 1, 126, 1, 1, 6, 1, 1, 21, 1, 8, 650, 543, 21, 1, 214, 6, 215, 267, 1, 584, 8, 1, 37, 1, 1, 30], [1, 1, 1, 599, 1, 220, 1, 1, 222, 1, 424, 184, 30, 30, 1, 30], [30, 30, 1, 1, 128, 223, 1, 30, 30, 1, 1, 1, 1, 638, 1, 1, 1, 17, 681, 30, 30, 1, 1, 9, 1, 1, 225], [1, 1, 1, 232, 8, 1, 21, 1, 30], [1, 6, 164, 577, 1, 1, 1, 1, 126, 1, 685, 6, 584, 8, 1], [1, 1, 1, 1, 1, 1], [128, 1, 652, 1, 1, 29, 1, 92, 599, 582, 530, 626, 267, 51, 184], [61], [1, 1, 596, 239, 240, 429, 95]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_to_index = {\n",
    "    word[0]: index + 2\n",
    "    for index, word in enumerate(vocab)\n",
    "}\n",
    "\n",
    "word_to_index[\"pad\"] = 0\n",
    "word_to_index[\"unk\"] = 1\n",
    "mu.log(\"len(word_to_index)\", len(word_to_index))\n",
    "\n",
    "encoded = []\n",
    "\n",
    "for line in tokenized:\n",
    "    temp = []\n",
    "    for w in line:\n",
    "        try:\n",
    "            temp.append(word_to_index[w])\n",
    "        except:\n",
    "            temp.append(word_to_index[\"unk\"])\n",
    "    encoded.append(temp)\n",
    "\n",
    "mu.log(\"len(encoded)\", len(encoded))\n",
    "mu.log(\"encoded[:10]\", encoded[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 길이가 다른 문장들을 모두 동일한 길이로 바꿔주는 패딩(padding) \n",
    "   - 이제 길이가 다른 리뷰들을 모두 동일한 길이로 바꿔주는 패딩 작업을 진행해보겠습니다. \n",
    "   - 앞서 단어 집합에 패딩을 위한 토큰인 'pad'를 추가했었습니다. \n",
    "   - 패딩 작업은 정해준 길이로 모든 샘플들의 길이를 맞춰주되, \n",
    "   - 길이가 정해준 길이보다 짧은 샘플들에는 'pad' 토큰을 추가하여 길이를 맞춰주는 작업입니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len : 63\n",
      "min_len : 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded[:3] : [[1, 594, 626, 267, 1, 1, 1, 1, 1, 1], [1, 6, 1, 1, 19, 1, 1, 623, 1, 1, 6, 1, 1, 164, 1, 1, 1, 1, 30, 1, 1, 1, 1, 623, 1, 1, 126, 1, 1, 6, 1, 1, 21, 1, 8, 650, 543, 21, 1, 214, 6, 215, 267, 1, 584, 8, 1, 37, 1, 1, 30], [1, 1, 1, 599, 1, 220, 1, 1, 222, 1, 424, 184, 30, 30, 1, 30]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "len_array = []\n",
    "for line in encoded:\n",
    "    len_array.append(len(line))\n",
    "\n",
    "max_len = max(len_array)\n",
    "mu.log(\"max_len\", max_len)\n",
    "min_len = min(len_array)\n",
    "mu.log(\"min_len\", min_len)\n",
    "\n",
    "plt.hist(len_array, bins=50, label=\"count of sample\")\n",
    "plt.xlabel(\"length of sample\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mu.log(\"encoded[:3]\", encoded[:3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

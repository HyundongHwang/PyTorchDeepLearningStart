{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import myutil as mu\n",
                "import os.path\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 영어/한국어 Word2Vec 훈련시키기 \n",
                "     - 영어와 한국어 훈련 데이터에 대해서 Word2Vec을 학습해보겠습니다. \n",
                "     - gensim 패키지에서 Word2Vec은 이미 구현되어져 있으므로, \n",
                "     - 별도로 Word2Vec을 구현할 필요없이 손쉽게 훈련시킬 수 있습니다 \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 영어 Word2Vec 만들기 \n",
                "   - 영어 데이터를 다운로드 받아 직접 Word2Vec 작업을 진행해보도록 하겠습니다. \n",
                "   - 영어로 된 코퍼스를 다운받아 전처리를 수행하고, \n",
                "   - 전처리한 데이터를 바탕으로 Word2Vec 작업을 진행하겠습니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 훈련 데이터 이해하기 \n",
                "   - zip 파일의 압축을 풀면 ted_en-20160408.xml이라는 파일을 얻을 수 있습니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import nltk\n",
                "\n",
                "nltk.download(\"punkt\")\n",
                "\n",
                "import urllib.request\n",
                "import zipfile\n",
                "from lxml import etree\n",
                "import re\n",
                "from nltk.tokenize import word_tokenize, sent_tokenize\n",
                "\n",
                "if not os.path.isfile(\".ted_en-20160408.xml\"):\n",
                "    urllib.request.urlretrieve(\n",
                "        url=\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\",\n",
                "        filename=\".ted_en-20160408.xml\"\n",
                "    )\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 훈련 데이터 전처리하기 \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "targetXML = open(\n",
                "    file=\".ted_en-20160408.xml\",\n",
                "    mode=\"r\",\n",
                "    encoding=\"UTF8\")\n",
                "\n",
                "target_text = etree.parse(targetXML)\n",
                "target_text_xpath = target_text.xpath('//content/text()')\n",
                "mu.log(\"len(target_text_xpath)\", len(target_text_xpath))\n",
                "mu.log(\"target_text_xpath[:5]\", target_text_xpath[:5])\n",
                "\n",
                "parse_text = \"\\n\".join(target_text_xpath)\n",
                "mu.log(\"len(parse_text)\", len(parse_text))\n",
                "mu.log(\"parse_text\", parse_text[:300])\n",
                "\n",
                "# 정규 표현식의 sub 모듈을 통해\n",
                "# content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
                "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
                "mu.log(\"len(content_text)\", len(content_text))\n",
                "mu.log(\"content_text\", content_text[:300])\n",
                "\n",
                "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
                "sent_text = sent_tokenize(content_text)\n",
                "mu.log(\"len(sent_text)\", len(sent_text))\n",
                "mu.log(\"sent_text\", sent_text[:5])\n",
                "\n",
                "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
                "normalized_text = []\n",
                "\n",
                "for string in sent_text:\n",
                "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
                "    normalized_text.append(tokens)\n",
                "\n",
                "mu.log(\"len(normalized_text)\", len(normalized_text))\n",
                "mu.log(\"normalized_text\", normalized_text[:5])\n",
                "\n",
                "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
                "result = []\n",
                "\n",
                "result = [\n",
                "    word_tokenize(sentence)\n",
                "    for sentence in normalized_text\n",
                "]\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 상위 3개 문장만 출력해보았는데 토큰화가 수행되었음을 볼 수 있습니다. \n",
                " - 이제 Word2Vec 모델에 텍스트 데이터를 훈련시킵니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "mu.log(\"len(result)\", len(result))\n",
                "mu.log(\"result\", result[:5])\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - Word2Vec 훈련시키기 \n",
                " ``` \n",
                " pip install gensim \n",
                " ``` \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from gensim.models import Word2Vec\n",
                "\n",
                "# size\n",
                "#   워드 벡터의 특징 값.\n",
                "#   즉, 임베딩 된 벡터의 차원.\n",
                "#   밀집벡터의 길이\n",
                "# window\n",
                "#   컨텍스트 윈도우 크기,\n",
                "#   함께 훈련한 연관단어 갯수\n",
                "# min_count\n",
                "#   단어 최소 빈도 수 제한\n",
                "#   (빈도가 적은 단어들은 학습하지 않는다.)\n",
                "# workers\n",
                "#   학습을 위한 프로세스 수\n",
                "# sg =\n",
                "#   0은 CBOW, 주변단어로 가운데 단어 맞추기,\n",
                "#   1은 Skip-gram, 가운데 단어로 주변단어 맞추기\n",
                "model = Word2Vec(\n",
                "    sentences=result,\n",
                "    size=100,\n",
                "    window=5,\n",
                "    min_count=5,\n",
                "    workers=4,\n",
                "    sg=0\n",
                ")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - Word2Vec는 입력한 단어에 대해서 가장 유사한 단어들을 출력하는 \n",
                " - model.wv.most_similar을 지원합니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "model_result = model.wv.most_similar(\"man\")\n",
                "mu.log(\"model_result\", model_result)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - Word2Vec 모델 저장하고 로드하기 \n",
                "   - 공들여 학습한 모델을 언제든 나중에 다시 사용할 수 있도록 컴퓨터 파일로 저장하고 다시 로드해보겠습니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from gensim.models import KeyedVectors\n",
                "\n",
                "model.wv.save_word2vec_format(\".eng_w2v\")\n",
                "loaded_model = KeyedVectors.load_word2vec_format(\".eng_w2v\")\n",
                "loaded_model_result = loaded_model.wv.most_similar(\"man\")\n",
                "mu.log(\"loaded_model_result\", loaded_model_result)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 한국어 Word2Vec 만들기 \n",
                "     - 위키피디아 한국어 덤프 파일을 다운받아서 한국어로 Word2Vec을 직접 진행해보도록 하겠습니다. \n",
                "     - 영어와 크게 다른 점은 없지만 한국어는 형태소 토큰화를 해야만 좋은 성능을 얻을 수 있습니다. \n",
                "     - 간단히 말해 형태소 분석기를 사용합니다. \n",
                " ``` \n",
                " curl https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2 -o .kowiki-latest-pages-articles.xml.bz2 \n",
                " pip install wikiextractor \n",
                " python -m wikiextractor.WikiExtractor .kowiki-latest-pages-articles.xml.bz2 \n",
                " pwsh \n",
                " Get-ChildItem text/wiki_* -Recurse | ForEach-Object { Get-Content $_.FullName | Add-Content .wiki_data.txt } \n",
                " ``` \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 훈련 데이터 전처리 하기 \n",
                "   - 파일이 정상적으로 저장되었는지 5개의 줄만 출력해보겠습니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "f = open(\".wiki_data.txt\", mode=\"r\", encoding=\"utf8\")\n",
                "\n",
                "i = 0\n",
                "\n",
                "while True:\n",
                "    line = f.readline()\n",
                "    if line != \"\\n\":\n",
                "        i = i + 1\n",
                "        mu.log(\"i\", i)\n",
                "        mu.log(\"line\", line)\n",
                "    if i == 5:\n",
                "        break\n",
                "\n",
                "f.close()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 이제 본격적으로 Word2Vec을 위한 학습 데이터를 만들어보겠습니다. \n",
                "     - 여기서는 형태소 분석기로 KoNLPy의 Okt를 사용하여 \n",
                "     - 명사만을 추출하여 훈련 데이터를 구성하겠습니다. \n",
                "     - 위 작업은 시간이 꽤 걸립니다. \n",
                "     - 훈련 데이터를 모두 만들었다면, \n",
                "     - 훈련 데이터의 길이를 확인해보겠습니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from konlpy.tag import Okt\n",
                "\n",
                "okt = Okt()\n",
                "fread = open(\".wiki_data.txt\", mode=\"r\", encoding=\"utf8\")\n",
                "n = 0\n",
                "result = []\n",
                "\n",
                "while True:\n",
                "    line = fread.readline()\n",
                "\n",
                "    if not line:\n",
                "        break\n",
                "\n",
                "    n = n + 1\n",
                "\n",
                "    if n % 1000 == 0:\n",
                "        mu.log(\"n\", n)\n",
                "\n",
                "    tokenlist = okt.pos(line, stem=True, norm=True)\n",
                "    temp = []\n",
                "\n",
                "    for word in tokenlist:\n",
                "        if word[1] in [\"Noun\"]:\n",
                "            temp.append(word[0])\n",
                "\n",
                "    if temp:\n",
                "        result.append(temp)\n",
                "\n",
                "fread.close()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 약 240만여개의 줄(line)이 명사 토큰화가 되어 저장되어 있는 상태입니다. \n",
                " - 이제 이를 Word2Vec으로 학습시킵니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "mu.log(\"len(result)\", len(result))\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - Word2Vec 훈련시키기 \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from gensim.models import Word2Vec\n",
                "\n",
                "model = Word2Vec(result, size=100, min_count=5, workers=4, sg=0)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 학습을 다했다면 이제 임의의 입력 단어로부터 유사한 단어들을 구해봅시다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "model_result = model.most_similar(\"대한민국\")\n",
                "mu.log(\"model_result\", model_result)\n",
                "\n",
                "model_result = model.most_similar(\"어벤져스\")\n",
                "mu.log(\"model_result\", model_result)\n",
                "\n",
                "model_result = model.most_similar(\"반도\")\n",
                "mu.log(\"model_result\", model_result)\n",
                "체\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 사전 훈련된 Word2Vec 임베딩(Pre-trained Word2Vec embedding) 소개 \n",
                "   - 위키피디아 등의 방대한 데이터로 사전에 훈련된 워드 임베딩(pre-trained word embedding vector)를 가지고 와서 \n",
                "   - 해당 벡터들의 값을 원하는 작업에 사용 할 수도 있습니다. \n",
                "   - 사전 훈련된 워드 임베딩을 가져와서 간단히 단어들의 유사도를 구해보는 실습을 해보겠습니다. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 영어 \n",
                "     - 구글이 제공하는 사전 훈련된(미리 학습되어져 있는) Word2Vec 모델을 사용하는 방법에 대해서 알아보도록 하겠습니다. \n",
                "     - 구글은 사전 훈련된 3백만 개의 Word2Vec 단어 벡터들을 제공합니다. \n",
                "     - 각 임베딩 벡터의 차원은 300입니다. \n",
                "     - gensim을 통해서 이 모델을 불러오는 건 매우 간단합니다. \n",
                "     - 이 모델을 다운로드하고 파일 경로를 기재하면 됩니다. \n",
                " ``` \n",
                " www https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit \n",
                " gzip -d ~/다운로드/GoogleNews-vectors-negative300.bin.gz \n",
                " mv ~/다운로드/GoogleNews-vectors-negative300.bin .GoogleNews-vectors-negative300.bin \n",
                " ``` \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "import gensim\n",
                "\n",
                "model = gensim.models.KeyedVectors.load_word2vec_format(\".GoogleNews-vectors-negative300.bin\", binary=True)\n",
                "mu.log(\"model.vectors.shape\", model.vectors.shape)\n",
                "\n",
                "mu.log(\"model.similarity('this', 'is')\", model.similarity(\"this\", \"is\"))\n",
                "mu.log(\"model.similarity('book', 'post')\", model.similarity(\"book\", \"post\"))\n",
                "mu.log(\"model['book']\", model['book'])\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                " - 한국어 \n",
                "     - 한국어의 미리 학습된 Word2Vec 모델은 박규병님의 깃허브 에 공개되어져 있습니다. \n",
                " \n",
                " ``` \n",
                " www https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view \n",
                " unzip ~/다운로드/ko.zip -d .ko \n",
                " mv ~/다운로드/ko.bin .ko.bin \n",
                " ``` \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "import gensim\n",
                "\n",
                "model = gensim.models.Word2Vec.load(\".ko/ko.bin\")\n",
                "result = model.mv.most_similar(\"강아지\")\n",
                "mu.log(\"result\", result)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 2
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython2",
            "version": "2.7.17"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
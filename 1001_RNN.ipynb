{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myutil as mu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 순환 신경망(Recurrent Neural Network, RNN) \n",
    "   - RNN(Recurrent Neural Network)은 시퀀스(Sequence) 모델입니다. \n",
    "   - 입력과 출력을 시퀀스 단위로 처리하는 모델입니다. \n",
    "   - 번역기를 생각해보면 입력은 번역하고자 하는 문장. \n",
    "   - 즉, 단어 시퀀스입니다. \n",
    "   - 출력에 해당되는 번역된 문장 또한 단어 시퀀스입니다. \n",
    "   - 이러한 시퀀스들을 처리하기 위해 고안된 모델들을 시퀀스 모델이라고 합니다. \n",
    "   - 그 중에서도 RNN은 딥 러닝에 있어 가장 기본적인 시퀀스 모델입니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " ![](https://wikidocs.net/images/page/22886/rnn_image3_ver2.PNG) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " ![](https://wikidocs.net/images/page/22886/rnn_image3.5.PNG) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " ![](https://wikidocs.net/images/page/22886/rnn_image3.7.PNG) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " ![](https://wikidocs.net/images/page/22886/rnn_image4_ver2.PNG) \n",
    " \n",
    " - 이를 식으로 표현하면 다음과 같습니다. \n",
    "   - 은닉층 : ht=tanh(Wxxt+Whht−1+b) \n",
    "   - 출력층 : yt=f(Wyht+b) \n",
    "   - 단, f는 비선형 활성화 함수 중 하나. \n",
    " \n",
    " xt  : (d×1) \n",
    " Wx : (Dh×d) \n",
    " Wh : (Dh×Dh) \n",
    " ht−1 : (Dh×1) \n",
    " b : (Dh×1) \n",
    " \n",
    " ![](https://wikidocs.net/images/page/22886/rnn_images4-5.PNG) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 파이썬으로 RNN 구현하기 \n",
    "   - 직접 Numpy로 RNN 층을 구현해보겠습니다. 앞서 메모리 셀에서 은닉 상태를 계산하는 식을 다음과 같이 정의하였습니다. \n",
    "   - ht=tanh(WxXt+Whht−1+b) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape : (10, 4)\n",
      "hidden_state_t.shape : (8,)\n",
      "Wx.shape : (8, 4)\n",
      "Wh.shape : (8, 8)\n",
      "b.shape : (8,)\n",
      "--------------------------------------------------------------------------------\n",
      "input_t.shape : (4,)\n",
      "output_t.shape : (8,)\n",
      "np.shape(total_hidden_states) : (1, 8)\n",
      "--------------------------------------------------------------------------------\n",
      "input_t.shape : (4,)\n",
      "output_t.shape : (8,)\n",
      "np.shape(total_hidden_states) : (2, 8)\n",
      "--------------------------------------------------------------------------------\n",
      "input_t.shape : (4,)\n",
      "output_t.shape : (8,)\n",
      "np.shape(total_hidden_states) : (3, 8)\n",
      "--------------------------------------------------------------------------------\n",
      "input_t.shape : (4,)\n",
      "output_t.shape : (8,)\n",
      "np.shape(total_hidden_states) : (4, 8)\n",
      "--------------------------------------------------------------------------------\n",
      "input_t.shape : (4,)\n",
      "output_t.shape : (8,)\n",
      "np.shape(total_hidden_states) : (5, 8)\n",
      "--------------------------------------------------------------------------------\n",
      "input_t.shape : (4,)\n",
      "output_t.shape : (8,)\n",
      "np.shape(total_hidden_states) : (6, 8)\n",
      "--------------------------------------------------------------------------------\n",
      "input_t.shape : (4,)\n",
      "output_t.shape : (8,)\n",
      "np.shape(total_hidden_states) : (7, 8)\n",
      "--------------------------------------------------------------------------------\n",
      "input_t.shape : (4,)\n",
      "output_t.shape : (8,)\n",
      "np.shape(total_hidden_states) : (8, 8)\n",
      "--------------------------------------------------------------------------------\n",
      "input_t.shape : (4,)\n",
      "output_t.shape : (8,)\n",
      "np.shape(total_hidden_states) : (9, 8)\n",
      "--------------------------------------------------------------------------------\n",
      "input_t.shape : (4,)\n",
      "output_t.shape : (8,)\n",
      "np.shape(total_hidden_states) : (10, 8)\n",
      "\n",
      "total_hidden_states : \n",
      "    [[0.58933317 0.96346287 0.9650367  0.84317441 0.8457019  0.74527129\n",
      "      0.67433462 0.95934645]\n",
      "     [0.9998232  0.99985309 0.99999906 0.99925049 0.99997748 0.99998647\n",
      "      0.99984515 0.99989304]\n",
      "     [0.99997956 0.99998763 0.99999998 0.99985194 0.99999887 0.99999848\n",
      "      0.99998794 0.99998569]\n",
      "     [0.99997689 0.99998094 0.99999996 0.99978402 0.99999836 0.99999817\n",
      "      0.9999839  0.99997741]\n",
      "     [0.99996156 0.9997966  0.99999949 0.99874985 0.99998351 0.99999519\n",
      "      0.9999236  0.99985174]\n",
      "     [0.99996002 0.99988042 0.99999976 0.99940397 0.99999378 0.99999538\n",
      "      0.99994855 0.99987907]\n",
      "     [0.99995953 0.99981407 0.99999959 0.99895424 0.99998691 0.99999557\n",
      "      0.99993021 0.99984497]\n",
      "     [0.99997236 0.99995905 0.99999992 0.99967812 0.99999651 0.99999791\n",
      "      0.99997467 0.99996147]\n",
      "     [0.99997243 0.99995329 0.99999991 0.99974651 0.99999622 0.99999773\n",
      "      0.99997346 0.9999669 ]\n",
      "     [0.99997369 0.9999799  0.99999997 0.99981702 0.99999872 0.99999807\n",
      "      0.99998399 0.9999713 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "timesteps = 10\n",
    "input_size = 4\n",
    "hidden_size = 8\n",
    "\n",
    "inputs = np.random.random((timesteps, input_size))\n",
    "mu.log(\"inputs.shape\", inputs.shape)\n",
    "\n",
    "hidden_state_t = np.zeros((hidden_size,))\n",
    "mu.log(\"hidden_state_t.shape\", hidden_state_t.shape)\n",
    "\n",
    "Wx = np.random.random((hidden_size, input_size))\n",
    "Wh = np.random.random((hidden_size, hidden_size))\n",
    "b = np.random.random((hidden_size,))\n",
    "\n",
    "mu.log(\"Wx.shape\", Wx.shape)\n",
    "mu.log(\"Wh.shape\", Wh.shape)\n",
    "mu.log(\"b.shape\", b.shape)\n",
    "\n",
    "total_hidden_states = []\n",
    "\n",
    "# 메모리 셀 동작\n",
    "for input_t in inputs:  # 각 시점에 따라서 입력값이 입력됨.\n",
    "    print(\"-\" * 80)\n",
    "    mu.log(\"input_t.shape\", input_t.shape)\n",
    "    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b)  # Wx * Xt + Wh * Ht-1 + b(bias)\n",
    "    mu.log(\"output_t.shape\", output_t.shape)\n",
    "    total_hidden_states.append(list(output_t))  # 각 시점의 은닉 상태의 값을 계속해서 축적\n",
    "    mu.log(\"np.shape(total_hidden_states)\",\n",
    "           np.shape(total_hidden_states))  # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)\n",
    "    hidden_state_t = output_t\n",
    "\n",
    "total_hidden_states = np.stack(total_hidden_states, axis=0)\n",
    "# 출력 시 값을 깔끔하게 해준다.\n",
    "\n",
    "# (timesteps, output_dim)의 크기. 이 경우 (10, 8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력.\n",
    "mu.log(\"total_hidden_states\", total_hidden_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 파이토치의 nn.RNN() \n",
    "   - 파이토치에서는 nn.RNN()을 통해서 RNN 셀을 구현합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs : \n",
      "    torch.Size([1, 10, 5]) tensor([[[0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "            ...\n",
      "\n",
      "_status.shape : torch.Size([1, 1, 8])\n",
      "outputs.shape : torch.Size([1, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 5\n",
    "hidden_size = 8\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "mu.log(\"inputs\", inputs)\n",
    "\n",
    "cell = nn.RNN(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "outputs, _status = cell(inputs)\n",
    "mu.log(\"_status.shape\", _status.shape)\n",
    "mu.log(\"outputs.shape\", outputs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 깊은 순환 신경망(Deep Recurrent Neural Network) \n",
    "   - 앞서 RNN도 다수의 은닉층을 가질 수 있다고 언급한 바 있습니다. \n",
    "   - 위의 그림은 순환 신경망에서 은닉층이 1개 더 추가되어 \n",
    "   - 은닉층이 2개인 깊은(deep) 순환 신경망의 모습을 보여줍니다. \n",
    "   - 위의 코드에서 첫번째 은닉층은 다음 은닉층에 모든 시점에 대해서 은닉 상태 값을 다음 은닉층으로 보내주고 있습니다. \n",
    "   - 깊은 순환 신경망을 파이토치로 구현할 때는 nn.RNN()의 인자인 num_layers에 값을 전달하여 층을 쌓습니다. \n",
    "   - 층이 2개인 깊은 순환 신경망의 경우, \n",
    "   - 앞서 실습했던 임의의 입력에 대해서 출력이 어떻게 달라지는지 확인해봅시다. \n",
    " ![](https://wikidocs.net/images/page/22886/rnn_image4.5_finalPNG.PNG) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_status.shape : torch.Size([2, 1, 8])\n",
      "outputs.shape : torch.Size([1, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.RNN(\n",
    "    input_size=5,\n",
    "    hidden_size=8,\n",
    "    num_layers=2,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "outputs, _status = cell(inputs)\n",
    "mu.log(\"_status.shape\", _status.shape)\n",
    "mu.log(\"outputs.shape\", outputs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 양방향 순환 신경망(Bidirectional Recurrent Neural Network) \n",
    "   - 양방향 순환 신경망은 시점 t에서의 출력값을 예측할 때 이전 시점의 데이터뿐만 아니라, \n",
    "   - 이후 데이터로도 예측할 수 있다는 아이디어에 기반합니다. \n",
    " \n",
    "   - 영어 빈칸 채우기 문제에 비유하여 보겠습니다. \n",
    " \n",
    " ``` \n",
    " Exercise is very effective at [          ] belly fat. \n",
    " \n",
    " 1) reducing \n",
    " 2) increasing \n",
    " 3) multiplying \n",
    " ``` \n",
    " \n",
    " - '운동은 복부 지방을 [ ] 효과적이다'라는 영어 문장이고, \n",
    " - 정답은 reducing(줄이는 것)입니다. \n",
    " - 그런데 위의 영어 빈 칸 채우기 문제를 잘 생각해보면 \n",
    " - 정답을 찾기 위해서는 이전에 나온 단어들만으로는 부족합니다. \n",
    " - 목적어인 belly fat(복부 지방)를 모르는 상태라면 정답을 결정하기가 어렵습니다. \n",
    " \n",
    " - 그래서 이전 시점의 데이터뿐만 아니라, \n",
    " - 이후 시점의 데이터도 힌트로 활용하기 위해서 고안된 것이 양방향 RNN입니다. \n",
    " \n",
    " ![](https://wikidocs.net/images/page/22886/rnn_image6_ver3.PNG) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_status.shape : torch.Size([4, 1, 8])\n",
      "outputs.shape : torch.Size([1, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# (batch_size, time_steps, input_size)\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.RNN(\n",
    "    input_size=5,\n",
    "    hidden_size=8,\n",
    "    num_layers=2,\n",
    "    batch_first=True,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "outputs, _status = cell(inputs)\n",
    "\n",
    "# (층의 개수 x 2, 배치 크기, 은닉 상태의 크기)\n",
    "mu.log(\"_status.shape\", _status.shape)\n",
    "\n",
    "# (배치 크기, 시퀀스 길이, 은닉 상태의 크기 x 2)\n",
    "mu.log(\"outputs.shape\", outputs.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

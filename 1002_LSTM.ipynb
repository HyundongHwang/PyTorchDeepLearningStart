{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myutil as mu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 장단기 메모리(Long Short-Term Memory, LSTM) \n",
    " - 바닐라 RNN의 한계 \n",
    "   - RNN은 출력 결과가 이전의 계산 결과에 의존한다는 것을 언급한 바 있습니다. \n",
    "   - 하지만 바닐라 RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있습니다. \n",
    "   - 바닐라 RNN의 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생합니다. \n",
    "   - 위의 그림은 첫번째 입력값인 x1의 정보량을 짙은 남색으로 표현했을 때, \n",
    "   - 색이 점차 얕아지는 것으로 시점이 지날수록 x1의 정보량이 손실되어가는 과정을 표현하였습니다. \n",
    "   - 뒤로 갈수록 x1의 정보량은 손실되고, \n",
    "   - 시점이 충분히 긴 상황에서는 x1의 전체 정보에 대한 영향력은 거의 의미가 없을 수도 있습니다. \n",
    " \n",
    " ![](https://wikidocs.net/images/page/22888/lstm_image1_ver2.PNG) \n",
    " \n",
    "   - 어쩌면 가장 중요한 정보가 시점의 앞 쪽에 위치할 수도 있습니다. \n",
    "   - RNN으로 만든 언어 모델이 다음 단어를 예측하는 과정을 생각해봅시다. \n",
    "   - 예를 들어 \n",
    "           - 모스크바에 여행을 왔는데 건물도 예쁘고 먹을 것도 맛있었어. \n",
    "           - 그런데 글쎄 직장 상사한테 전화가 왔어. \n",
    "           - 어디냐고 묻더라구 그래서 나는 말했지. 저 여행왔는데요. 여기 ___ \n",
    "   - 다음 단어를 예측하기 위해서는 장소 정보가 필요합니다. \n",
    "   - 그런데 장소 정보에 해당되는 단어인 '모스크바'는 앞에 위치하고 있고, \n",
    "   - RNN이 충분한 기억력을 가지고 있지 못한다면 \n",
    "   - 다음 단어를 엉뚱하게 예측합니다. \n",
    " \n",
    "   - 이를 장기 의존성 문제(the problem of Long-Term Dependencies)라고 합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " \n",
    " \n",
    " - 바닐라 RNN 내부 열어보기 \n",
    " \n",
    " ![](https://wikidocs.net/images/page/22888/vanilla_rnn_ver2.PNG) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " \n",
    " \n",
    " - LSTM(Long Short-Term Memory) \n",
    "   - LSTM은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 \n",
    "   - 불필요한 기억을 지우고, 기억해야할 것들을 정합니다. \n",
    "   - 요약하면 LSTM은 은닉 상태(hidden state)를 계산하는 식이 전통적인 RNN보다 조금 더 복잡해졌으며 \n",
    "   - 셀 상태(cell state)라는 값을 추가하였습니다. \n",
    "   - 위의 그림에서는 t시점의 셀 상태를 Ct로 표현하고 있습니다. \n",
    "   - LSTM은 RNN과 비교하여 긴 시퀀스의 입력을 처리하는데 탁월한 성능을 보입니다. \n",
    " \n",
    " \n",
    " ![](https://wikidocs.net/images/page/22888/vaniila_rnn_and_different_lstm_ver2.PNG) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_status : \n",
      "    (tensor([[[-0.0010,  0.0635, -0.0202, -0.2451,  0.1507, -0.1340,  0.1141,\n",
      "              -0.1272]],\n",
      "    \n",
      "            [[-0.0000, -0.0000, -0.7627, -0.0000,  0.0000, -0.0000,  0.7616,\n",
      "               0.5592]],\n",
      "    \n",
      "            [[ 0.0394,  0.0085, -0.2195,  0.1038,  0.0623, -0.2369,  0.0552,\n",
      "               0.0596]],\n",
      "    \n",
      "            [[ 0.0939, -0.0309,  0.0752, -0.2426,  0.2971,  0.0086, -0.0114,\n",
      "              -0.1855]]], grad_fn=<StackBackward>), tensor([[[-0.0028,  0.1438, -0.0351, -0.5131,  0.2902, -0.2161,  0.2085,\n",
      "              -0.2965]],\n",
      "    \n",
      "            [[-0.0000, -0.1122, -1.0028, -0.0000,  0.4957, -0.0589,  1.0000,\n",
      "               0.6317]],\n",
      "    \n",
      "            [[ 0.0761,  0.0156, -0.5696,  0.1908,  0.1273, -0.5516,  0.1368,\n",
      "               0.1201]],\n",
      "    \n",
      "            [[ 0.2935, -0.1370,  0.2135, -0.5717,  0.4091,  0.0113, -0.0329,\n",
      "              -0.5225]]], grad_fn=<StackBackward>))\n",
      "\n",
      "outputs.shape : torch.Size([1, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.LSTM(\n",
    "    input_size=5,\n",
    "    hidden_size=8,\n",
    "    num_layers=2,\n",
    "    batch_first=True,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "outputs, _status = cell(inputs)\n",
    "mu.log(\"_status\", _status)\n",
    "mu.log(\"outputs.shape\", outputs.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myutil as mu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " - 장단기 메모리(Long Short-Term Memory, LSTM) \n",
    " - 바닐라 RNN의 한계 \n",
    "   - RNN은 출력 결과가 이전의 계산 결과에 의존한다는 것을 언급한 바 있습니다. \n",
    "   - 하지만 바닐라 RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있습니다. \n",
    "   - 바닐라 RNN의 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생합니다. \n",
    "   - 위의 그림은 첫번째 입력값인 x1의 정보량을 짙은 남색으로 표현했을 때, \n",
    "   - 색이 점차 얕아지는 것으로 시점이 지날수록 x1의 정보량이 손실되어가는 과정을 표현하였습니다. \n",
    "   - 뒤로 갈수록 x1의 정보량은 손실되고, \n",
    "   - 시점이 충분히 긴 상황에서는 x1의 전체 정보에 대한 영향력은 거의 의미가 없을 수도 있습니다. \n",
    " \n",
    " ![](https://wikidocs.net/images/page/22888/lstm_image1_ver2.PNG) \n",
    " \n",
    "   - 어쩌면 가장 중요한 정보가 시점의 앞 쪽에 위치할 수도 있습니다. \n",
    "   - RNN으로 만든 언어 모델이 다음 단어를 예측하는 과정을 생각해봅시다. \n",
    "   - 예를 들어 \n",
    "           - 모스크바에 여행을 왔는데 건물도 예쁘고 먹을 것도 맛있었어. \n",
    "           - 그런데 글쎄 직장 상사한테 전화가 왔어. \n",
    "           - 어디냐고 묻더라구 그래서 나는 말했지. 저 여행왔는데요. 여기 ___ \n",
    "   - 다음 단어를 예측하기 위해서는 장소 정보가 필요합니다. \n",
    "   - 그런데 장소 정보에 해당되는 단어인 '모스크바'는 앞에 위치하고 있고, \n",
    "   - RNN이 충분한 기억력을 가지고 있지 못한다면 \n",
    "   - 다음 단어를 엉뚱하게 예측합니다. \n",
    " \n",
    "   - 이를 장기 의존성 문제(the problem of Long-Term Dependencies)라고 합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " \n",
    " \n",
    " - 바닐라 RNN 내부 열어보기 \n",
    " \n",
    " ![](https://wikidocs.net/images/page/22888/vanilla_rnn_ver2.PNG) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " \n",
    " \n",
    " - LSTM(Long Short-Term Memory) \n",
    "   - LSTM은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 \n",
    "   - 불필요한 기억을 지우고, 기억해야할 것들을 정합니다. \n",
    "   - 요약하면 LSTM은 은닉 상태(hidden state)를 계산하는 식이 전통적인 RNN보다 조금 더 복잡해졌으며 \n",
    "   - 셀 상태(cell state)라는 값을 추가하였습니다. \n",
    "   - 위의 그림에서는 t시점의 셀 상태를 Ct로 표현하고 있습니다. \n",
    "   - LSTM은 RNN과 비교하여 긴 시퀀스의 입력을 처리하는데 탁월한 성능을 보입니다. \n",
    " \n",
    " \n",
    " ![](https://wikidocs.net/images/page/22888/vaniila_rnn_and_different_lstm_ver2.PNG) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_status : \n",
      "    (tensor([[[-0.1801,  0.0219,  0.1179,  0.1689,  0.1576, -0.2875,  0.0066,\n",
      "               0.1528]],\n",
      "    \n",
      "            [[ 0.0606,  0.0493, -0.0809, -0.0182, -0.1066, -0.1291,  0.1448,\n",
      "               0.1613]],\n",
      "    \n",
      "            [[-0.1667,  0.1381,  0.1355, -0.1337,  0.0430,  0.0890,  0.2594,\n",
      "              -0.1190]],\n",
      "    \n",
      "            [[ 0.2408,  0.0391,  0.1419, -0.0986,  0.0628,  0.0864,  0.1629,\n",
      "               0.1636]]], grad_fn=<StackBackward>), tensor([[[-0.3101,  0.0458,  0.2119,  0.3039,  0.2770, -0.6521,  0.0111,\n",
      "               0.3002]],\n",
      "    \n",
      "            [[ 0.0966,  0.1135, -0.1677, -0.0341, -0.2061, -0.2185,  0.2636,\n",
      "               0.3413]],\n",
      "    \n",
      "            [[-0.3047,  0.2273,  0.2521, -0.3640,  0.0869,  0.2216,  0.4177,\n",
      "              -0.2799]],\n",
      "    \n",
      "            [[ 0.5804,  0.0748,  0.2922, -0.1945,  0.1205,  0.2145,  0.3214,\n",
      "               0.3641]]], grad_fn=<StackBackward>))\n",
      "\n",
      "outputs.shape : torch.Size([1, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.LSTM(\n",
    "    input_size=5,\n",
    "    hidden_size=8,\n",
    "    num_layers=2,\n",
    "    batch_first=True,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "outputs, _status = cell(inputs)\n",
    "mu.log(\"_status\", _status)\n",
    "mu.log(\"outputs.shape\", outputs.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
